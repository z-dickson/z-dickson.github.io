[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "7. Going against the Grain: Climate Change as a Wedge Issue for the Radical Right\nwith Sara B Hobolt\nAbstract: In search of electoral gains, political parties mobilize issues to divide existing coalitions. We argue that by adopting a distinctively adversarial stance, radical right-wing parties have increasingly politicized climate change policies as a wedge issue. This strategy challenges the mainstream party consensus and seeks to mobilize voter concerns over green initiatives. Relying on state-of-the-art multilingual language models, we empirically examine nearly half a million press releases from 82 political parties across 9 European democracies to support this argument. Our findings demonstrate that the radical right’s oppositional climate policy rhetoric diverges significantly from the mainstream consensus. Survey data further reveals climate policy skepticism among voters across the political spectrum, highlighting the mobilizing potential of climate policies as a wedge issue. This research advances our understanding of issue competition and the politicization of climate change.\n6. Elite Cues and Mass Non-compliance\nwith Sara B Hobolt\nSlide presentation – 2023 PolMeth Europe\nAbstract: Elite cues have been shown to influence public attitudes and behaviors. But to what extent can political elites encourage mass non-compliance simply by naming the geographic location of a subset of the population? In this paper, we study the effects of elite cues on non-compliant behavior during the COVID-19 pandemic, focusing on a series of controversial tweets sent by US President Donald Trump calling for the “liberation” of Minnesota, Virginia and Michigan from COVID-19 restrictions at the height of the pan- demic. Leveraging the fact that these messages referred to three specific US states, we adopt a generalized difference-in-difference approach relying on spatial variation to identify the causal effects of the targeted cues. Our analysis finds that the President’s messages inspired mass non-compliance in the forms of an increase in public mobility, a decrease in adherence to stay-at- home restrictions, and an increase in criminal activity in the days following. Treatment effect heterogeneity further demonstrates Trump’s outsized in fluence on mobility in Republican counties and on criminal activity among White individuals. Our findings have broad implications for our understand- ing of the capacity of elites to motivate mass non-compliant behavior.\n5. The Limits of Partisan Motivated Reasoning in Congress\nwith Tevfik Murat Yildirim\nSlide presentation – 2023 MPSA\nAbstract: Downplaying the level of risk associated with COVID-19—along with opposition to public health measures aimed at containing the virus—became a rallying cry for certain legislators in the US Congress throughout the pandemic. Yet, to what degree are elites ``blind partisans,” willing to take on the positions of their parties regardless of potential consequences? In this article, we argue that elites’ strategic motivations transcend partisan motivated reasoning when individuals realize a risk that is otherwise misperceived. We demonstrate our argument in the context of the COVID-19 pandemic by examining the effects of COVID-19 infection on opposition to COVID-19 mitigation policies among United States’ legislators. After training a large language model to identify opposition messages from legislators, we adopt a staggered difference-in-difference design and use both Bayesian and frequentist perspectives to illustrate that COVID-19 infection caused a reduction of around 32 percent in legislators’ expressed opposition to measures aimed at reducing the spread of COVID-19. Our findings highlight the limits of partisan motivated reasoning for an issue as salient as COVID-19 and in elites as polarized as US Members of Congress.\n4. The Gender Gap in Elite-Voter Responsiveness Online\nAbstract: The extent to which descriptive representation furthers the substantive representation of women have been demonstrated in countless contexts. Yet, we know less about how female representatives respond dynamically to the shifting priorities of the women they represent. In this article, I examine representatives’ dynamic responsiveness to public policy priorities in the United States and United Kingdom. By combining hundreds of repeated public opinion surveys and the universe of representatives’ social media messages on Twitter, I show that women are individually more responsive than their male colleagues to both men and women’s policy priorities in both countries. The results of the analysis provide further evidence of the link between descriptive and substantive representation by illustrating that when left to act in an individual capacity, women go beyond furthering ``women’s issues’’ by displaying greater dynamic responsiveness to the public’s changing policy demands.\n3. Elite Legislators and Unequal Representation in the UK\nSlide presentation – 2023 PSA Parliaments\nAbstract: Studies identifying inequality in political representation in liberal democracies have become increasingly common. Yet, the extent to which this deficit is driven by the social class of elected representatives remains unclear. In this article, I study the effects of social class on legislative responsiveness in the United Kingdom by utilizing MPs’ attendance at one of the two Oxbridge universities – Oxford and Cambridge – as an encompassing proxy for social class. After combining 284 repeated public opinion surveys and classifying the universe of MPs’ questions and motions in the House of Commons from 2015-2023, I present evidence from multiple designs and estimation strategies that suggests that social class indeed constrains responsiveness. The results quantify unequal responsiveness in the United Kingdom and contribute to the literature on inequality in political representation.\n2. Youth Suffrage and Legislative Responsiveness to Climate Change: Evidence from Scotland\n\nSlide presentation – 2022 ECPR Joint Sessions\nAbstract: While the vast majority of countries around the world require that voters be 18 years of age to cast a ballot, advocates of extending the franchise to include individuals aged 16 and over point to greater engagement, and countries that have lowered the voting age to 16 have witnessed increased political participation compared to some older groups. While a handful of studies have looked at the effects of lowering the voting age to 16 on political knowledge or participation, we have almost no understanding of how youth enfranchisement influences legislative behavior. In this article, we adopt a quasi-experimental design and the synthetic control method to examine legislative responsiveness following 2014 legislation that lowered the minimum voting age from 18 to 16 in Scotland. Analysis of 1.8 million legislative speeches from the House of Commons and the Scottish Parliament over the course of two decades indicates that lowering the voting age had only marginal effects on legislative responsiveness to younger voters. We conclude that enfranchisement of 16- and 17 year-olds does not go far enough to address a lack of substantive representation of young people.\n1. The Relative Online Influence of NGOs, Corporations and the Media\nWorking Paper SSRN\nAbstract: It is well-established that various actors compete to draw the attention of politicians to issues they view as important. Yet, given the numerous channels through which different actors attempt to influence the political agenda, we know less about the relative influence of these different actors. This article examines the degree to which non-governmental organizations (NGOs), multinational corporations (MNCs) and top UK media organizations influence the issue attention of parliamentarians in the UK House of Commons on the same playing field – social media. Using a dataset that includes the universe of social media messages from the top-100 British MNCs and NGOs, top British media organizations, and messages sent by elected MPs, I rely on Natural Language Processing (NLP) methods to measure the dynamic attention of each actor to different salient issues. After creating and validating an issue dictionary based on semi-supervised machine learning, I use vector autoregression (VAR) models to estimate the relative influence that each actor has on MPs’ issue attention online. Findings reveal broad support for the influence of the media and corporations.\n\nOther words\nAcademic:\n\nPhD Thesis slides\nJohn Smith Centre Blog: MPs’ Gender and Responsiveness in the House of Commons\n\nNon-academic:\n\nSo Hormonal. 2020. Monstrous Regiment Publishing. Horgan, E. and Dickson, Z.\nWere women MPs more responsive to women’s priorities during the pandemic?"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html",
    "href": "assets/newspaper_sentiment_attention_tutorial.html",
    "title": "How to guide – Language model and keyword searches",
    "section": "",
    "text": "Code\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nMounted at /content/drive"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#in-this-notebook-ill-demonstrate-how-to-use-the-multilingual-language-model-i-created-to-get-the-sentiment-of-newspaper-headlines.-ill-also-show-how-to-create-time-series-data-according-to-topics-using-keywords.",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#in-this-notebook-ill-demonstrate-how-to-use-the-multilingual-language-model-i-created-to-get-the-sentiment-of-newspaper-headlines.-ill-also-show-how-to-create-time-series-data-according-to-topics-using-keywords.",
    "title": "How to guide – Language model and keyword searches",
    "section": "In this notebook, I’ll demonstrate how to use the multilingual language model I created to get the sentiment of newspaper headlines. I’ll also show how to create time-series data according to topics using keywords.",
    "text": "In this notebook, I’ll demonstrate how to use the multilingual language model I created to get the sentiment of newspaper headlines. I’ll also show how to create time-series data according to topics using keywords.\nFeel free to copy this notebook and use it for yourself. If you do not already have python installed already on your computer and you don’t want to install it (how to check), then you can use this notebook in Google Colab.\nThis option can be especially appealing if you want to use the language model on lots of text, because Google Colab allows you to use a GPU, which increases the speed of the model significantly. I’ll demonstrate how to do this at the end also.\n\n\nI created a new dataset that combines all the newspapers collected for all five countries. I’ve also cleaned this data so that it is ready to use exactly like I do in this notebook. Feel free to use it as you wish. It’s available for download in Dropbox\n\n\n\nCode\n##### if you're using google colab, you'll need to change the file location of the dataset to the appropriate location in order\n#### There are a few ways to do this -- one would be to mount your own google drive. Another would be to upload to newspaper file in the Colab notebook in the left hand column.\n\n\n\n\n\nimport pandas as pd # import necessary library\nimport numpy as np # import necessary library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default='colab'\n\n\npd.set_option('display.max_columns', 50)  ## set max columns to display as 50\n\ndf = pd.read_csv('./newspaper_data_all.csv')  ## read in the dataset. See the first comment in this box of code\ndf.date = pd.to_datetime(df.date)  # set date column to pandas version of date\n\ndf.newspaper = df.newspaper.str.replace('_', ' ').str.title() ## these are just changes to the names of the newspapers for presentation purposes\ndf.country = df.country.str.title().str.replace('Uk','UK') ## these are just changes to the names of the newspapers for presentation purposes\n\n\n\n\nNote: if you have trouble importing the newspapers dataset, see the following tutorial.\n## First five rows of the dataset:\n\n\nCode\ndf.head()\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nlink\ntitle\nnewspaper\ncountry\n\n\n\n\n0\n2020-01-01\nhttps://sport.fakt.pl/inne-sporty/david-stern-...\nZmarł były komisarz ligi NBA David Stern\nFakt\nPoland\n\n\n1\n2020-01-01\nhttps://www.fakt.pl/polityka/sylwester-polityk...\n\"Grzeczny\" sylwester polityków. Tylko Jaki się...\nFakt\nPoland\n\n\n2\n2020-01-01\nhttps://www.fakt.pl/wydarzenia/polska/zaglada-...\nZagłada ptaków w Polsce. W środę zabijały je w...\nFakt\nPoland\n\n\n3\n2020-01-01\nhttps://sport.fakt.pl/pilka-nozna/nietypowa-ci...\nNietypowa cieszynka piłkarzy Cracovii. Przebra...\nFakt\nPoland\n\n\n4\n2020-01-01\nhttps://www.fakt.pl/plotki/z-kim-ania-z-rolnik...\nZ kim Ania z „Rolnik szuka żony” spędziła sylw...\nFakt\nPoland"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#ill-visualise-some-basic-descriptive-statistics-for-the-dataset",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#ill-visualise-some-basic-descriptive-statistics-for-the-dataset",
    "title": "How to guide – Language model and keyword searches",
    "section": "I’ll visualise some basic descriptive statistics for the dataset",
    "text": "I’ll visualise some basic descriptive statistics for the dataset\nArticles per newspaper:\n\n\nCode\ndf.groupby('newspaper').size()\n\n\nnewspaper\nAbc Spain               50666\nBild                   204491\nDe Telegraaf           176781\nDe Welt                 42199\nEl Mundo                39738\nEl Pais                 67419\nFakt                    89627\nGazeta Wyborcza         27390\nGuardian               149828\nNrc                     49361\nRzeczpospolita          20647\nSuddeutsche Zeitung    586495\nUk Sun                 128994\nUk Times                32062\nVolkskrant              62579\ndtype: int64\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme('notebook')\nplt.style.use('fivethirtyeight')\n\ndf.groupby('newspaper').size().plot(kind='bar')\nplt.title('Number of Newspapers')\n\n\nText(0.5, 1.0, 'Number of Newspapers')\n\n\n\n\n\nArticles per newspaper, per country\n\n\nCode\ndf.groupby(['country','newspaper']).size()\n\n\ncountry      newspaper          \nGermany      Bild                   204491\n             De Welt                 42199\n             Suddeutsche Zeitung    586495\nNetherlands  De Telegraaf           176781\n             Nrc                     49361\n             Volkskrant              62579\nPoland       Fakt                    89627\n             Gazeta Wyborcza         27390\n             Rzeczpospolita          20647\nSpain        Abc Spain               50666\n             El Mundo                39738\n             El Pais                 67419\nUK           Guardian               149828\n             Uk Sun                 128994\n             Uk Times                32062\ndtype: int64\n\n\n\n\nCode\nx = pd.DataFrame(df.groupby(['country','newspaper']).size()).reset_index().rename(columns={0:'count'})\n\nplt.figure(figsize=(10,5))\n\nsns.barplot(x='newspaper', y='count', hue='country', data=x)\nplt.xticks(rotation=90)\n\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n [Text(0, 0, 'Bild'),\n  Text(1, 0, 'De Welt'),\n  Text(2, 0, 'Suddeutsche Zeitung'),\n  Text(3, 0, 'De Telegraaf'),\n  Text(4, 0, 'Nrc'),\n  Text(5, 0, 'Volkskrant'),\n  Text(6, 0, 'Fakt'),\n  Text(7, 0, 'Gazeta Wyborcza'),\n  Text(8, 0, 'Rzeczpospolita'),\n  Text(9, 0, 'Abc Spain'),\n  Text(10, 0, 'El Mundo'),\n  Text(11, 0, 'El Pais'),\n  Text(12, 0, 'Guardian'),\n  Text(13, 0, 'Uk Sun'),\n  Text(14, 0, 'Uk Times')])"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#in-the-next-cell-ill-run-the-same-code-to-import-the-model.-youll-likely-need-to-install-the-necessary-libraries-on-your-machine-or-in-google-colab.-these-can-easily-be-done-by-running-the-following-code-in-a-single-cell",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#in-the-next-cell-ill-run-the-same-code-to-import-the-model.-youll-likely-need-to-install-the-necessary-libraries-on-your-machine-or-in-google-colab.-these-can-easily-be-done-by-running-the-following-code-in-a-single-cell",
    "title": "How to guide – Language model and keyword searches",
    "section": "In the next cell, I’ll run the same code to import the model. You’ll likely need to install the necessary libraries on your machine or in google colab. These can easily be done by running the following code in a single cell:",
    "text": "In the next cell, I’ll run the same code to import the model. You’ll likely need to install the necessary libraries on your machine or in google colab. These can easily be done by running the following code in a single cell:\npip install transformers\n\n\nCode\npip install transformers\n\n\nCollecting transformers\n  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 60.1 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\nCollecting huggingface-hub&lt;1.0,&gt;=0.16.4 (from transformers)\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 33.3 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nCollecting tokenizers&lt;0.15,&gt;=0.14 (from transformers)\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 104.7 MB/s eta 0:00:00\nCollecting safetensors&gt;=0.3.1 (from transformers)\n  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 74.3 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.16.4-&gt;transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.16.4-&gt;transformers) (4.5.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.3.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.6)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2023.7.22)\nInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\nSuccessfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n\n\n\n\nCode\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline,TFAutoModelForSequenceClassification\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"z-dickson/multilingual_sentiment_newspaper_headlines\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"z-dickson/multilingual_sentiment_newspaper_headlines\")\nsentiment_classifier = TextClassificationPipeline(tokenizer=tokenizer, model=model, device=0) ## if you're using colab, change the runtime type and add 'device=0' in the parentheses to use a GPU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome layers from the model checkpoint at z-dickson/multilingual_sentiment_newspaper_headlines were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertForSequenceClassification were initialized from the model checkpoint at z-dickson/multilingual_sentiment_newspaper_headlines.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training."
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#now-that-we-have-imported-the-model-into-our-notebook-and-have-created-the-classifier-we-can-classify-different-headlines.",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#now-that-we-have-imported-the-model-into-our-notebook-and-have-created-the-classifier-we-can-classify-different-headlines.",
    "title": "How to guide – Language model and keyword searches",
    "section": "Now that we have imported the model into our notebook and have created the classifier, we can classify different headlines.",
    "text": "Now that we have imported the model into our notebook and have created the classifier, we can classify different headlines.\n\nI’ll classify the following headline in each of the five languages used. The headline is just a random headline from the dataset:\nIn German, the headline is as follows:\n\nDüsseldorf: Mann von jungem Messerstecher nach Streit schwer verletzt\nEnglish: Dusseldorf: Man seriously injured by young knife man after argument\n\n\n\nWe’ll pass the headline into the classifier in German using\n#example\nsentiment_classifier('Mann von jungem Messerstecher nach Streit schwer verletzt')\nThe result is as follows:\n\n\nCode\nsentiment_classifier('Mann von jungem Messerstecher nach Streit schwer verletzt')\n\n\n[{'label': 'negative', 'score': 0.9974070191383362}]"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#the-estimated-sentiment-is-negative-and-the-model-is-very-confident-indicating-a-score-of-.99xxx.-what-were-interested-in-is-the-sentiment-label-negative",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#the-estimated-sentiment-is-negative-and-the-model-is-very-confident-indicating-a-score-of-.99xxx.-what-were-interested-in-is-the-sentiment-label-negative",
    "title": "How to guide – Language model and keyword searches",
    "section": "The estimated sentiment is negative, and the model is very confident indicating a score of .99xxx. What we’re interested in is the sentiment label negative",
    "text": "The estimated sentiment is negative, and the model is very confident indicating a score of .99xxx. What we’re interested in is the sentiment label negative"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#lets-try-the-same-thing-in-english-using-the-following",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#lets-try-the-same-thing-in-english-using-the-following",
    "title": "How to guide – Language model and keyword searches",
    "section": "Let’s try the same thing in English, using the following:",
    "text": "Let’s try the same thing in English, using the following:\n#example\nsentiment_classifier('Man seriously injured by young knife man after argument')\n\nThe only thing we change is the text, which I simply translated to English using google translate in a browser\nThe result is nearly identical\n\n\nCode\nsentiment_classifier('Man seriously injured by young knife man after argument')\n\n\n[{'label': 'negative', 'score': 0.998826801776886}]\n\n\nNext, I’ll work through some examples that you might want to use in an analysis"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#for-the-example-well-generate-a-list-of-headlines-at-random.-the-dataset-will-include-ten-rows-with-different-headlines",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#for-the-example-well-generate-a-list-of-headlines-at-random.-the-dataset-will-include-ten-rows-with-different-headlines",
    "title": "How to guide – Language model and keyword searches",
    "section": "For the example, we’ll generate a list of headlines at random. The dataset will include ten rows with different headlines",
    "text": "For the example, we’ll generate a list of headlines at random. The dataset will include ten rows with different headlines\nhere’s the dataset:\n\n\nCode\n\n\nheadline_example = df.sample(10)\n\nheadline_example\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nlink\ntitle\nnewspaper\ncountry\n\n\n\n\n1649067\n2021-11-03\nhttps://www.bild.de/sport/mehr-sport/baseball/...\nBaseball: Atlanta Braves gewinnen World Series...\nBild\nGermany\n\n\n1125040\n2021-01-21\nhttps://www.sueddeutsche.de/sport/basketball-t...\nTheis-Gala reicht Celtics nicht zum Sieg über ...\nSuddeutsche Zeitung\nGermany\n\n\n1345511\n2021-12-23\nhttps://www.sueddeutsche.de/politik/italien-pa...\nFürsorgliches Italien\nSuddeutsche Zeitung\nGermany\n\n\n784752\n2021-05-06\n/economia/2021-05-06/peaje-en-las-autovias-fon...\nPeajes en las autovías, fondo para los ERTE y ...\nEl Pais\nSpain\n\n\n646337\n2022-06-25\nhttps://www.telegraaf.nl/nieuws/1431146744/lim...\nLimburgse pastoor ’die seksfilmpje toonde’ nie...\nDe Telegraaf\nNetherlands\n\n\n1030380\n2020-08-23\nhttps://www.sueddeutsche.de/muenchen/starnberg...\nBermuda-Dreieck in der Mittelkonsole\nSuddeutsche Zeitung\nGermany\n\n\n556442\n2021-02-12\nhttps://www.telegraaf.nl/sport/1236508470/dubb...\nDubbel schaatsgoud voor Oranje: ’We moesten wa...\nDe Telegraaf\nNetherlands\n\n\n1155656\n2021-03-01\nhttps://www.sueddeutsche.de/muenchen/erding/er...\nOnline-Diskussion zur Seenotrettung\nSuddeutsche Zeitung\nGermany\n\n\n290975\n2022-06-27\nhttps://www.theguardian.com/world/2022/jun/27/...\nNato to put 300,000 troops on high alert in re...\nGuardian\nUK\n\n\n1259656\n2021-08-21\nhttps://www.sueddeutsche.de/panorama/kriminali...\nVandalen verwüsten Kinder- und Jugendzentrum i...\nSuddeutsche Zeitung\nGermany"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#ill-write-a-function-below-that-will-take-in-a-dataset-like-the-one-we-have-above-and-return-the-same-dataset-but-it-will-include-columns-for-the-sentiment-of-each-article-title.-the-only-thing-you-need-to-do-to-get-the-result-is-to-provide-the-name-of-the-dataset-in-the-form-of-the-above-in-brackets-with-the-name-of-the-function-sentiment_analysis.",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#ill-write-a-function-below-that-will-take-in-a-dataset-like-the-one-we-have-above-and-return-the-same-dataset-but-it-will-include-columns-for-the-sentiment-of-each-article-title.-the-only-thing-you-need-to-do-to-get-the-result-is-to-provide-the-name-of-the-dataset-in-the-form-of-the-above-in-brackets-with-the-name-of-the-function-sentiment_analysis.",
    "title": "How to guide – Language model and keyword searches",
    "section": "I’ll write a function below that will take in a dataset like the one we have above, and return the same dataset but it will include columns for the sentiment of each article title. The only thing you need to do to get the result is to provide the name of the dataset (in the form of the above) in brackets with the name of the function (sentiment_analysis).",
    "text": "I’ll write a function below that will take in a dataset like the one we have above, and return the same dataset but it will include columns for the sentiment of each article title. The only thing you need to do to get the result is to provide the name of the dataset (in the form of the above) in brackets with the name of the function (sentiment_analysis)."
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#for-example",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#for-example",
    "title": "How to guide – Language model and keyword searches",
    "section": "For example:",
    "text": "For example:\n#example\nsentiment_analysis(headline_example)\n\nreturns the following:\n\n\nCode\nfrom logging import raiseExceptions\n\n\ndef sentiment_analysis(dataset, date_start=None, date_end=None):\n  x = dataset.copy()\n  try:\n    if date_start != None:\n      x.date = pd.to_datetime(x.date)\n      x = x.loc[(x.date &lt;= f'{date_end}') & (x.date &gt;= f'{date_start}')]\n    sentiment_list = []\n    sentiment = sentiment_classifier(x.title.to_list())\n    sentiment_list.append(sentiment)\n    x['sentiment'] = sentiment\n    x['sentiment_label'] = x.sentiment.apply(lambda x: x['label'])\n    return x\n  except TypeError:\n    print(\"ERROR: ensure date_start and date_end are in the format Year-Month-Day. For example: 2020-01-08\")\n\n\n\n\n\n\n\nCode\nsentiment_analysis(headline_example)\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nlink\ntitle\nnewspaper\ncountry\nsentiment\nsentiment_label\n\n\n\n\n1649067\n2021-11-03\nhttps://www.bild.de/sport/mehr-sport/baseball/...\nBaseball: Atlanta Braves gewinnen World Series...\nBild\nGermany\n{'label': 'positive', 'score': 0.6868038177490...\npositive\n\n\n1125040\n2021-01-21\nhttps://www.sueddeutsche.de/sport/basketball-t...\nTheis-Gala reicht Celtics nicht zum Sieg über ...\nSuddeutsche Zeitung\nGermany\n{'label': 'neutral', 'score': 0.7599111199378967}\nneutral\n\n\n1345511\n2021-12-23\nhttps://www.sueddeutsche.de/politik/italien-pa...\nFürsorgliches Italien\nSuddeutsche Zeitung\nGermany\n{'label': 'neutral', 'score': 0.8194842338562012}\nneutral\n\n\n784752\n2021-05-06\n/economia/2021-05-06/peaje-en-las-autovias-fon...\nPeajes en las autovías, fondo para los ERTE y ...\nEl Pais\nSpain\n{'label': 'neutral', 'score': 0.6340370178222656}\nneutral\n\n\n646337\n2022-06-25\nhttps://www.telegraaf.nl/nieuws/1431146744/lim...\nLimburgse pastoor ’die seksfilmpje toonde’ nie...\nDe Telegraaf\nNetherlands\n{'label': 'neutral', 'score': 0.8677178025245667}\nneutral\n\n\n1030380\n2020-08-23\nhttps://www.sueddeutsche.de/muenchen/starnberg...\nBermuda-Dreieck in der Mittelkonsole\nSuddeutsche Zeitung\nGermany\n{'label': 'neutral', 'score': 0.9753379821777344}\nneutral\n\n\n556442\n2021-02-12\nhttps://www.telegraaf.nl/sport/1236508470/dubb...\nDubbel schaatsgoud voor Oranje: ’We moesten wa...\nDe Telegraaf\nNetherlands\n{'label': 'neutral', 'score': 0.976171612739563}\nneutral\n\n\n1155656\n2021-03-01\nhttps://www.sueddeutsche.de/muenchen/erding/er...\nOnline-Diskussion zur Seenotrettung\nSuddeutsche Zeitung\nGermany\n{'label': 'positive', 'score': 0.568717896938324}\npositive\n\n\n290975\n2022-06-27\nhttps://www.theguardian.com/world/2022/jun/27/...\nNato to put 300,000 troops on high alert in re...\nGuardian\nUK\n{'label': 'negative', 'score': 0.9975405931472...\nnegative\n\n\n1259656\n2021-08-21\nhttps://www.sueddeutsche.de/panorama/kriminali...\nVandalen verwüsten Kinder- und Jugendzentrum i...\nSuddeutsche Zeitung\nGermany\n{'label': 'negative', 'score': 0.9977375268936...\nnegative"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#now-we-have-two-new-columns-with-the-sentiment-with-label-and-score-and-the-sentiment-label",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#now-we-have-two-new-columns-with-the-sentiment-with-label-and-score-and-the-sentiment-label",
    "title": "How to guide – Language model and keyword searches",
    "section": "Now we have two new columns with the sentiment (with label and score) and the sentiment label",
    "text": "Now we have two new columns with the sentiment (with label and score) and the sentiment label"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#well-call-the-function-get_proportions_by_keyword.-heres-an-example-using-the-keyword-border-the-uk-sun-and-a-monthly-frequency.-well-name-the-topic-borders",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#well-call-the-function-get_proportions_by_keyword.-heres-an-example-using-the-keyword-border-the-uk-sun-and-a-monthly-frequency.-well-name-the-topic-borders",
    "title": "How to guide – Language model and keyword searches",
    "section": "We’ll call the function get_proportions_by_keyword. Here’s an example using the keyword “border”, the UK Sun, and a monthly frequency. We’ll name the topic “Borders”",
    "text": "We’ll call the function get_proportions_by_keyword. Here’s an example using the keyword “border”, the UK Sun, and a monthly frequency. We’ll name the topic “Borders”\nHere’s the code to do the above:\n# example\nget_proportion_by_keyword(df, 'border', 'Uk Sun', 'M', 'Borders')\nwhich returns the following\n\n\nCode\nget_proportion_by_keyword(df, 'border', 'Uk Sun', 'M', 'Borders')\n\n\n\n  \n    \n\n\n\n\n\n\ndate\ntotal_headlines\nissue_headlines\nattention\ntopic\nnewspaper\n\n\n\n\n0\n2020-01-31\n1095\n2\n0.001826\nBorders\nUk Sun\n\n\n1\n2020-02-29\n872\n3\n0.003440\nBorders\nUk Sun\n\n\n2\n2020-03-31\n1101\n11\n0.009991\nBorders\nUk Sun\n\n\n3\n2020-04-30\n1240\n1\n0.000806\nBorders\nUk Sun\n\n\n4\n2020-05-31\n1155\n4\n0.003463\nBorders\nUk Sun\n\n\n5\n2020-06-30\n1094\n14\n0.012797\nBorders\nUk Sun\n\n\n6\n2020-07-31\n1203\n6\n0.004988\nBorders\nUk Sun\n\n\n7\n2020-08-31\n2260\n6\n0.002655\nBorders\nUk Sun\n\n\n8\n2020-09-30\n2419\n8\n0.003307\nBorders\nUk Sun\n\n\n9\n2020-10-31\n2295\n6\n0.002614\nBorders\nUk Sun\n\n\n10\n2020-11-30\n2377\n8\n0.003366\nBorders\nUk Sun\n\n\n11\n2020-12-31\n2301\n15\n0.006519\nBorders\nUk Sun\n\n\n12\n2021-01-31\n2384\n21\n0.008809\nBorders\nUk Sun\n\n\n13\n2021-02-28\n2372\n10\n0.004216\nBorders\nUk Sun\n\n\n14\n2021-03-31\n2741\n9\n0.003283\nBorders\nUk Sun\n\n\n15\n2021-04-30\n2596\n14\n0.005393\nBorders\nUk Sun\n\n\n16\n2021-05-31\n2328\n10\n0.004296\nBorders\nUk Sun\n\n\n17\n2021-06-30\n2318\n0\n0.000000\nBorders\nUk Sun\n\n\n18\n2021-07-31\n2264\n5\n0.002208\nBorders\nUk Sun\n\n\n19\n2021-08-31\n2026\n4\n0.001974\nBorders\nUk Sun\n\n\n20\n2021-09-30\n2035\n6\n0.002948\nBorders\nUk Sun\n\n\n21\n2021-10-31\n1972\n2\n0.001014\nBorders\nUk Sun\n\n\n22\n2021-11-30\n1852\n7\n0.003780\nBorders\nUk Sun\n\n\n23\n2021-12-31\n1694\n6\n0.003542\nBorders\nUk Sun\n\n\n24\n2022-01-31\n1815\n7\n0.003857\nBorders\nUk Sun\n\n\n25\n2022-02-28\n1659\n16\n0.009644\nBorders\nUk Sun\n\n\n26\n2022-03-31\n1780\n6\n0.003371\nBorders\nUk Sun\n\n\n27\n2022-04-30\n1425\n5\n0.003509\nBorders\nUk Sun\n\n\n28\n2022-05-31\n1506\n6\n0.003984\nBorders\nUk Sun"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#now-we-can-take-the-new-dataset-and-plot-monthly-attention-to-borders-by-the-sun",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#now-we-can-take-the-new-dataset-and-plot-monthly-attention-to-borders-by-the-sun",
    "title": "How to guide – Language model and keyword searches",
    "section": "Now we can take the new dataset and plot monthly attention to borders by the Sun",
    "text": "Now we can take the new dataset and plot monthly attention to borders by the Sun\n\n\nCode\nx = get_proportion_by_keyword(df, 'border', 'Uk Sun', 'M', 'Borders')\npio.renderers.default='colab'\n#sns.relplot(x='date', y='attention', hue='topic', data=x, kind='line', aspect=2) ## same plot in seaborn\n\nfig = px.line(x, x=\"date\", y=\"attention\", color='topic',markers=True, title='Attention to borders by UK Sun', height=500)\nfig.update_layout(template='plotly_white')\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"black\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"black\",\n    font_size = 18,\n    legend_title_font_color=\"black\",\n    template='plotly_white',\n    showlegend=True,\n    xaxis_title = 'Date',\n    width=1200)\nfig.show(include_plotlyjs=True)"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#we-can-similarly-get-the-weekly-values-as-well",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#we-can-similarly-get-the-weekly-values-as-well",
    "title": "How to guide – Language model and keyword searches",
    "section": "We can similarly get the weekly values as well:",
    "text": "We can similarly get the weekly values as well:\n\n\nCode\nx = get_proportion_by_keyword(df, 'border', 'Uk Sun', 'W', 'Borders')\n\n#sns.relplot(x='date', y='attention', hue='topic', data=x, kind='line', aspect=2)\n\nfig = px.line(x, x=\"date\", y=\"attention\", color='topic',markers=True, title='Attention to borders by UK Sun', height=500)\nfig.update_layout(template='plotly_white')\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"black\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"black\",\n    font_size = 18,\n    legend_title_font_color=\"black\",\n    template='plotly_white',\n    showlegend=True,\n    xaxis_title = 'Date',\n    width=1200)\nfig.show(include_plotlyjs=True)"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#in-this-example-ill-show-how-to-use-the-functions-from-this-notebook-to-compare-the-attention-of-different-newspapers-to-different-issues.-this-one-is-a-bit-more-complex-so-feel-free-to-reach-out-if-you-have-trouble.",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#in-this-example-ill-show-how-to-use-the-functions-from-this-notebook-to-compare-the-attention-of-different-newspapers-to-different-issues.-this-one-is-a-bit-more-complex-so-feel-free-to-reach-out-if-you-have-trouble.",
    "title": "How to guide – Language model and keyword searches",
    "section": "In this example, I’ll show how to use the functions from this notebook to compare the attention of different newspapers to different issues. This one is a bit more complex, so feel free to reach out if you have trouble.",
    "text": "In this example, I’ll show how to use the functions from this notebook to compare the attention of different newspapers to different issues. This one is a bit more complex, so feel free to reach out if you have trouble.\n\n\nSay I want to get the weekly attention of all the German newspapers to vaccines during the pandemic. I also want to compare how the sentiment differed between the newspapers.\nFirst I need to identify the keywords related to vaccines. I’ll use a few for each language and store them as lists:\n## example:\ngerman_keywords = ['impfung', 'impfimpfung', 'impfen']\n\n\nCode\n\ngerman_keywords = ['impfung', 'impfimpfung', 'impfen']\n\n\nThen I can create new datasets for all the newspapers in Germany. I’ll do this by using the get_titles_by_keyword function and pass in the country so that it returns all the German newspapers:\n#example:\nget_titles_by_keyword(dataset = df, keywords = german_keywords, country = 'Germany')\nwhich provides the following:\n\n\nCode\nget_titles_by_keyword(dataset = df, keywords = german_keywords, country = 'Germany')\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nlink\ntitle\nnewspaper\ncountry\n\n\n\n\n867760\n2022-12-29\nhttps://www.sueddeutsche.de/bayern/bayern-coro...\nBilanz : Bayerns Impfzentren schließen zum Jah...\nSuddeutsche Zeitung\nGermany\n\n\n867804\n2022-12-08\nhttps://www.sueddeutsche.de/gesundheit/china-c...\nSars-CoV-2 : Heftige Infektionswelle in China ...\nSuddeutsche Zeitung\nGermany\n\n\n867873\n2022-11-22\nhttps://www.sueddeutsche.de/politik/impfpflich...\nCorona-Pandemie : Pfleger und Ärzte brauchen k...\nSuddeutsche Zeitung\nGermany\n\n\n867881\n2022-11-17\nhttps://www.sueddeutsche.de/gesundheit/kinder-...\nStiko-Empfehlung : Gesunde Kleinkinder brauche...\nSuddeutsche Zeitung\nGermany\n\n\n867885\n2022-11-16\nhttps://www.sueddeutsche.de/gesundheit/guertel...\nNebenwirkungen der Covid-Impfung? : Verdachtsf...\nSuddeutsche Zeitung\nGermany\n\n\n...\n...\n...\n...\n...\n...\n\n\n1684082\n2022-04-13\nhttps://www.bild.de/bild-plus/ratgeber/psychol...\nPsychische Form von Gewalt: Schweigen ist grau...\nBild\nGermany\n\n\n1693094\n2022-05-24\nhttps://www.bild.de/ratgeber/2022/ratgeber/cov...\nCovid19-Empfehlung der Stiko - Gesunde Kinder ...\nBild\nGermany\n\n\n1696690\n2022-06-11\nhttps://www.bild.de/bild-plus/ratgeber/kind-fa...\nKinder: Was Sie nach dem Schimpfen immer tun s...\nBild\nGermany\n\n\n1698376\n2022-06-19\nhttps://www.bild.de/ratgeber/2022/ratgeber/usa...\nUSA impfen Kleinste gegen Corona - Corona-Piks...\nBild\nGermany\n\n\n1699624\n2022-06-25\nhttps://www.bild.de/regional/leipzig/leipzig-n...\nImpfen: Neuer Stichtags-Ärger bei Kliniken und...\nBild\nGermany\n\n\n\n\n\n5218 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nI’ll store that dataset of german headlines about vaccines with a new variable called: germany_vaccines\n#example\ngermany_vaccines = get_titles_by_keyword(dataset = df, keywords = german_keywords, country = 'Germany')\nWe’ll then use the new dataset to get the sentiment of each article using the sentiment_analysis function from above and storing the dataset as another variable called sentiment_germany_vaccines:\n#example\nsentiment_germany_vaccines = sentiment_analysis(germany_vaccines)\nThis will take a few minutes, because we are using the language model to classify each of the 5,000+ headlines about vaccines in Germany.\nHere’s what we’ll get if we view that new dataset:\n\n\nCode\ngermany_vaccines = get_titles_by_keyword(dataset = df, keywords = german_keywords, country = 'Germany')\n\ngermany_vaccines = germany_vaccines.sample(1000) ## we'll only use a sample of 1000 so it doesn't take a long time to classify all the headlines\n\nsentiment_germany_vaccines = sentiment_analysis(germany_vaccines)\n\n\n\n\nCode\nsentiment_germany_vaccines\n\n\n\n  \n    \n\n\n\n\n\n\ndate\nlink\ntitle\nnewspaper\ncountry\nsentiment\nsentiment_label\n\n\n\n\n1246095\n2021-07-30\nhttps://www.sueddeutsche.de/muenchen/fuerstenf...\nImpfen zeigt deutliche Wirkung\nSuddeutsche Zeitung\nGermany\n{'label': 'neutral', 'score': 0.5086002349853516}\nneutral\n\n\n1310215\n2021-10-26\nhttps://www.sueddeutsche.de/gesundheit/gesundh...\nUSA-Reisen ab November möglich nach Impfung\nSuddeutsche Zeitung\nGermany\n{'label': 'neutral', 'score': 0.5342729687690735}\nneutral\n\n\n1332218\n2021-12-02\nhttps://www.sueddeutsche.de/muenchen/fuerstenf...\nDie Impfung wirkt\nSuddeutsche Zeitung\nGermany\n{'label': 'positive', 'score': 0.6801331639289...\npositive\n\n\n1315560\n2021-11-01\nhttps://www.sueddeutsche.de/gesundheit/gesundh...\nAuffrischungsimpfungen in den meisten Heimen z...\nSuddeutsche Zeitung\nGermany\n{'label': 'neutral', 'score': 0.7856541275978088}\nneutral\n\n\n1335496\n2021-11-29\nhttps://www.sueddeutsche.de/gesundheit/gesundh...\nPolizei startet mit Corona-Auffrischungsimpfungen\nSuddeutsche Zeitung\nGermany\n{'label': 'negative', 'score': 0.8802963495254...\nnegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1577247\n2020-12-18\nhttps://www.bild.de/bild-plus/ratgeber/2020/ra...\nFragen zum Impfplan - Bin ich dick genug für e...\nBild\nGermany\n{'label': 'neutral', 'score': 0.9369633793830872}\nneutral\n\n\n1583904\n2021-01-15\nhttps://www.bild.de/regional/muenchen/muenchen...\nGesundheitsminister Holetschek - „Das Impfen i...\nBild\nGermany\n{'label': 'positive', 'score': 0.5605814456939...\npositive\n\n\n1148468\n2021-02-25\nhttps://www.sueddeutsche.de/gesundheit/gesundh...\nSieben-Tage-Inzidenz sinkt nur langsam: 168.00...\nSuddeutsche Zeitung\nGermany\n{'label': 'negative', 'score': 0.993061363697052}\nnegative\n\n\n1251331\n2021-08-06\nhttps://www.sueddeutsche.de/gesundheit/kranken...\nCorona-Auffrischimpfung: Krankenhäuser bereite...\nSuddeutsche Zeitung\nGermany\n{'label': 'positive', 'score': 0.7920066118240...\npositive\n\n\n1586770\n2021-01-27\nhttps://www.bild.de/news/inland/news-inland/di...\nDiese Rentner wollen endlich ihre Corona-Impfu...\nBild\nGermany\n{'label': 'positive', 'score': 0.7067196369171...\npositive\n\n\n\n\n\n1000 rows × 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nIf we want to see the totals of how many positive, negative and neutral headlines about vaccines by newspapers:\n\n\nCode\nsentiment_germany_vaccines.groupby(['newspaper', 'sentiment_label']).size().reset_index()\n\n\n\n  \n    \n\n\n\n\n\n\nnewspaper\nsentiment_label\n0\n\n\n\n\n0\nBild\nnegative\n95\n\n\n1\nBild\nneutral\n85\n\n\n2\nBild\npositive\n59\n\n\n3\nDe Welt\nnegative\n32\n\n\n4\nDe Welt\nneutral\n33\n\n\n5\nDe Welt\npositive\n8\n\n\n6\nSuddeutsche Zeitung\nnegative\n189\n\n\n7\nSuddeutsche Zeitung\nneutral\n281\n\n\n8\nSuddeutsche Zeitung\npositive\n218\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\nCode\nsentiment_germany_vaccines.groupby(['newspaper', 'sentiment_label']).size().unstack().plot(kind='bar', stacked=True, figsize=(8,4))\n\n\n&lt;Axes: xlabel='newspaper'&gt;"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#what-if-we-want-to-get-the-attention-of-the-german-newspapers-to-vaccines-throughout-the-pandemic",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#what-if-we-want-to-get-the-attention-of-the-german-newspapers-to-vaccines-throughout-the-pandemic",
    "title": "How to guide – Language model and keyword searches",
    "section": "What if we want to get the attention of the German newspapers to vaccines throughout the pandemic?",
    "text": "What if we want to get the attention of the German newspapers to vaccines throughout the pandemic?\nFor that, we just need to pass in the german_keywords to the get_proportion_by_keywords function. Because the function requires the newspapaer name, we’ll create three separate datasets for each of the German newspapers:\nRemember, the German newspapers are: Suddeutsche Zeitung, De Welt, and Bild\nattention_SZ = get_proportion_by_keyword(df, keywords = german_keywords, newspaper = 'Suddeutsche Zeitung', frequency = 'M', topic = 'Vaccines')\nattention_DW = get_proportion_by_keyword(df, keywords = german_keywords, newspaper = 'De Welt', frequency = 'M', topic = 'Vaccines')\nattention_Bild = get_proportion_by_keyword(df, keywords = german_keywords, newspaper = 'Bild', frequency = 'M', topic = 'Vaccines')\nNow we can plot the attention to vaccines by any of the newspapers:\n\n\nCode\nattention_SZ = get_proportion_by_keyword(df, keywords = german_keywords, newspaper = 'Suddeutsche Zeitung', frequency = 'M', topic = 'Vaccines')\nattention_DW = get_proportion_by_keyword(df, keywords = german_keywords, newspaper = 'De Welt', frequency = 'M', topic = 'Vaccines')\nattention_Bild = get_proportion_by_keyword(df, keywords = german_keywords, newspaper = 'Bild', frequency = 'M', topic = 'Vaccines')\n\n\n\n\nCode\n#sns.relplot(x='date', y='attention', hue='newspaper', data=attention_SZ, kind='line', aspect=2)\n\nfig = px.line(attention_SZ, x=\"date\", y=\"attention\", color='newspaper',markers=True, title='Attention', height=500)\nfig.update_layout(template='plotly_white')\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"black\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"black\",\n    font_size = 18,\n    legend_title_font_color=\"black\",\n    template='plotly_white',\n    showlegend=True,\n    xaxis_title = 'Date',\n    width=1200)\nfig.show(include_plotlyjs=True)\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\nWhat if we want to plot them all in the same graph?\nWe can combine the datasets and then re-create the same plot:\n# combine dataset:\ngerman_newspapers_combined = pd.concat([attention_SZ, attention_DW, attention_Bild])\n# reset index\ngerman_newspapers_combined.reset_index(inplace=True)\n# recreate plot:\nsns.relplot(x='date', y='attention', hue='newspaper', data = german_newspapers_combined, kind='line', aspect=2)\n# add title (optional)\nplt.title('Attention to Vaccines in German Newspapers')\nwhich gives the following result:\n\n\nCode\ngerman_newspapers_combined = pd.concat([attention_SZ, attention_DW, attention_Bild])\n## reset index\ngerman_newspapers_combined.reset_index(inplace=True)\n#recreate plot:\nfig = px.line(german_newspapers_combined, x=\"date\", y=\"attention\", color='newspaper',markers=True, title='Attention to Vaccines in German Newspapers', height=500)\n\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"black\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"black\",\n    font_size = 18,\n    legend_title_font_color=\"black\",\n    template='plotly_white',)\nfig.show(include_plotlyjs=True)"
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#in-this-final-tutorial-ill-get-dynamic-attention-to-borders-using-keywords-as-well-as-dynamic-sentiment.-i-got-a-little-lazy-with-adding-comments-sorry.",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#in-this-final-tutorial-ill-get-dynamic-attention-to-borders-using-keywords-as-well-as-dynamic-sentiment.-i-got-a-little-lazy-with-adding-comments-sorry.",
    "title": "How to guide – Language model and keyword searches",
    "section": "In this final tutorial, I’ll get dynamic attention to ‘borders’ using keywords, as well as dynamic sentiment. I got a little lazy with adding comments (sorry).",
    "text": "In this final tutorial, I’ll get dynamic attention to ‘borders’ using keywords, as well as dynamic sentiment. I got a little lazy with adding comments (sorry).\n\n\nCode\ngerman = ['grenz','schengen','Reisebeschränkung', 'Reiseverbot', 'Einreiseverbot', 'Mobilitätsbeschränkung',  'Schlagbaum']\npolish = ['granica', 'Schengen', 'ograniczenie', 'zakaz',  'podróży', 'Zakazy', 'mobilności', 'szlaban']\nspanish = ['front','schengen','restriccion','prohibición', 'prohibiciones','movilidad']\ndutch = ['grens','schengen','reisbeperking', 'reisverbod', 'inreisverbod', 'mobilititeitsbeperking',  'slagboom']\nenglish = ['border','schengen','restriction','prohibition', 'prohibitions','mobility','travel ban','entry ban']\n\ndic = {'Poland': 'polish', 'Germany': 'german', 'Spain': 'spanish', 'Netherlands': 'dutch', 'UK': 'english'}\ndf['language'] = df['country'].map(dic)\n\n\n\n\nCode\nlanguages = ['german', 'polish', 'spanish', 'dutch', 'english']\n\n\nattn= pd.DataFrame()\n\nfor language in languages:\n    for newspaper in df.loc[df.language == language].newspaper.unique():\n        try:\n            attention = get_proportion_by_keyword(df, keywords = eval(language), newspaper = newspaper, frequency = 'M', topic = 'Borders')\n            attention['country'] = language\n            attn = pd.concat([attn, attention])\n        except TypeError:\n            pass\n\n\nattn=attn.reset_index(drop=True)\nlang_to_country = {'german': 'Germany', 'polish': 'Poland', 'spanish': 'Spain', 'dutch': 'Netherlands', 'english': 'UK'}\nattn['country'] = attn['country'].map(lang_to_country)\n\n\nYour query did not return anything. Make sure that the country, newspaper, and dates are correct. Did you give a start date that is later than the end date?\n --- \nThe optional newspapers are: ['Fakt' 'Rzeczpospolita' 'Gazeta Wyborcza' 'Uk Times' 'Guardian' 'Uk Sun'\n 'Nrc' 'De Telegraaf' 'Volkskrant' 'El Mundo' 'El Pais' 'Abc Spain'\n 'Suddeutsche Zeitung' 'De Welt' 'Bild']\n\n\n\n\n\nCode\nx = attn.groupby(['country','date']).mean().reset_index()\n\nfig = px.line(x, x=\"date\", y=\"attention\", color='country',markers=True, title='Attention to Borders in European Newspapers', height=500)\n\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"black\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"black\",\n    font_size = 18,\n    legend_title_font_color=\"black\",\n    template='seaborn',\n)\n\n\nfig.show(include_plotlyjs=True)\n\n#sns.relplot(x='date', y='attention', hue='country', data=attn, kind='line', aspect=2)\n#plt.title('Attention to Borders in European Newspapers')\n\n\nFutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nCode\n### Let's get the sentiment of the headlines that contain the keywords over time\n\n\nlanguages = ['german', 'polish', 'spanish', 'dutch', 'english']\n\n\nattn1= pd.DataFrame()\n\nfor newspaper in df.newspaper.unique():\n    try:\n        lang = df.loc[df.newspaper == newspaper].language.unique()[0]\n        attention = get_titles_by_keyword(df, keywords = eval(lang), newspaper = newspaper)\n        attention['newspaper'] = newspaper\n        attn1 = pd.concat([attn1, attention])\n    except TypeError:\n        pass\n\n\nYour query did not return anything. Make sure that the country, newspaper, and dates are correct. Did you give a start date that is later than the end date?\n --- \nThe optional newspapers are: ['Fakt' 'Rzeczpospolita' 'Gazeta Wyborcza' 'Uk Times' 'Guardian' 'Uk Sun'\n 'Nrc' 'De Telegraaf' 'Volkskrant' 'El Mundo' 'El Pais' 'Abc Spain'\n 'Suddeutsche Zeitung' 'De Welt' 'Bild']\n\n\n\n\nCode\nattn1 = attn1.reset_index(drop=True)\n\nattn1 = attn1.sample(2000, replace=True) ### we'll only take a sample of 1000 of these headlines so it doesn't take too long\n\nattn1 = sentiment_analysis(attn1)\n\n\n\n\nCode\nattn1['sentiment_centered'] = pd.Categorical(attn1.sentiment_label, categories=['negative', 'neutral', 'positive'], ordered=True).codes - 1\n\n\n\n\nCode\nattn1.groupby(['newspaper', 'sentiment_label']).size().unstack().plot(kind='bar', stacked=True, figsize=(8,4))\nplt.title('Sentiment of Headlines about Borders in European Newspapers')\n\n\nText(0.5, 1.0, 'Sentiment of Headlines about Borders in European Newspapers')\n\n\n\n\n\n\n\nCode\nattn2 = attn1.groupby(['newspaper','country', pd.Grouper(key='date', freq='1M')]).mean().reset_index()\nattn2 = attn2.query('date &gt; \"2020-01-01\" and date &lt; \"2022-05-31\"')\n\n\nFutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n\n\n\n\n\nCode\nx = attn2.groupby(['country', 'date']).mean().reset_index()\n\nx['Sentiment'] = x.sentiment_centered\nx['Country'] = x.country\n\nfig = px.line(x, x='date',y='Sentiment',color='Country',markers=True, title='Dynamic Sentiment of Headlines about Borders in European Newspapers', height=500,\n              color_discrete_sequence=px.colors.qualitative.T10)\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"white\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"white\",\n    font_size = 18,\n    legend_title_font_color=\"white\",\n    template='plotly_white'\n)\n\nfig.update_layout(\n    font_family=\"Courier New\",\n    font_color=\"black\",\n    title_font_family=\"Courier New\",\n    title_font_color=\"black\",\n    font_size = 18,\n    legend_title_font_color=\"black\",\n    template='plotly_white',\n    showlegend=True,\n    xaxis_title = 'Date',\n    width=1200)\n\n\nfig.update_traces(\n    marker=dict(\n        size=10),\n    line=dict(\n        width=3\n    )\n)\n\n\nfig.show(include_plotlyjs=True)\n#sns.relplot(x='date', y='sentiment_centered', hue='country', data=attn2, kind='line', aspect=2)\n#plt.title('Dynamic Sentiment of Headlines about Borders in European Newspapers')\n\n\nFutureWarning:\n\nThe default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function."
  },
  {
    "objectID": "assets/newspaper_sentiment_attention_tutorial.html#how-do-i-use-the-gpu-in-google-colab-if-i-need-to-get-the-sentiment-for-a-lot-of-newspaper-headlines",
    "href": "assets/newspaper_sentiment_attention_tutorial.html#how-do-i-use-the-gpu-in-google-colab-if-i-need-to-get-the-sentiment-for-a-lot-of-newspaper-headlines",
    "title": "How to guide – Language model and keyword searches",
    "section": "How do I use the GPU in Google Colab if I need to get the sentiment for a lot of newspaper headlines?",
    "text": "How do I use the GPU in Google Colab if I need to get the sentiment for a lot of newspaper headlines?\nClassifying the sentiment of the headlines can take a long time. For example, the ~5k German vaccines headlines took about 5 minutes. This poses a big challenge if we want to classify many more headlines.\nOne alternative is to use a GPU for the matrix multiplication that occurs billions of times when using transformers models. CPUs are not very good at this task, so using a GPU can be MUCH faster. Luckily, Google Colab allows free use of GPUs. Google Colab is probably the best way to run this entire notebook just because Python seems to be challenging for a lot of people to get install locally, so even if you don’t want to use a GPU, Google Colab might be the best option.\nGoogle has a tutorial on how to use a GPU in Colab here. Once you enable the GPU, the only change you’ll need to make to the language is model is to add in device=0 when defining the classifier. This would look like the following:\n\nsentiment_classifier = TextClassificationPipeline(tokenizer=tokenizer, model=m1, device=0)  ## add in device=0 to use the GPU\ninstead of this:\nsentiment_classifier = TextClassificationPipeline(tokenizer=tokenizer, model=m1)  ## uses the slower CPU"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Data & ML",
    "section": "",
    "text": "z-dickson/bart-large-cnn-climate-change-summarization\n\n\n\n\n\n\nz-dickson/US_politicians_covid_skepticism\n\n\n\n\n\n\n\nz-dickson/CAP_multilingual\n\n\n\n\n\n\n\nz-dickson/multilingual_sentiment_newspaper_headlines\n\n\n\n\n\n\n\n~2 million newspaper headlines from Pl, DE, UK, ES & NL 2020-2022: [dropbox]\nUK Parliamentary Statutory Instruments: 1970-2021 [github]\nParliamentary Bills - Dáil Éireann (Ireland): 1950-2020 [github]\nParliamentary Bills - New Zealand Parliament: 1900-2020 [github]"
  },
  {
    "objectID": "datasets.html#facebook-bart-large-cnn-sequence-to-sequence-model-trained-to-summarise-policy-positions-from-party-press-releases",
    "href": "datasets.html#facebook-bart-large-cnn-sequence-to-sequence-model-trained-to-summarise-policy-positions-from-party-press-releases",
    "title": "Data & ML",
    "section": "",
    "text": "z-dickson/bart-large-cnn-climate-change-summarization"
  },
  {
    "objectID": "datasets.html#vinaibertweet-large-model-trained-to-predict-opposition-to-covid-19-policies-from-us-congressmembers-tweets",
    "href": "datasets.html#vinaibertweet-large-model-trained-to-predict-opposition-to-covid-19-policies-from-us-congressmembers-tweets",
    "title": "Data & ML",
    "section": "",
    "text": "z-dickson/US_politicians_covid_skepticism"
  },
  {
    "objectID": "datasets.html#bert-base-multilingual-cased-trained-to-predict-the-cap-issue-codes-of-political-text-i.e.-bills-speeches-tweets-etc.",
    "href": "datasets.html#bert-base-multilingual-cased-trained-to-predict-the-cap-issue-codes-of-political-text-i.e.-bills-speeches-tweets-etc.",
    "title": "Data & ML",
    "section": "",
    "text": "z-dickson/CAP_multilingual"
  },
  {
    "objectID": "datasets.html#bert-base-multilingual-cased-sentiment-model-trained-on-polish-english-spanish-dutch-and-german-newspaper-headlines.",
    "href": "datasets.html#bert-base-multilingual-cased-sentiment-model-trained-on-polish-english-spanish-dutch-and-german-newspaper-headlines.",
    "title": "Data & ML",
    "section": "",
    "text": "z-dickson/multilingual_sentiment_newspaper_headlines"
  },
  {
    "objectID": "datasets.html#datasets",
    "href": "datasets.html#datasets",
    "title": "Data & ML",
    "section": "",
    "text": "~2 million newspaper headlines from Pl, DE, UK, ES & NL 2020-2022: [dropbox]\nUK Parliamentary Statutory Instruments: 1970-2021 [github]\nParliamentary Bills - Dáil Éireann (Ireland): 1950-2020 [github]\nParliamentary Bills - New Zealand Parliament: 1900-2020 [github]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zachary Dickson",
    "section": "",
    "text": "I’m currently a postdoctoral research fellow at the London School of Economics on the COVIDEU project. I’m also an affiliate at the  LSE Data Science Institute. My research focuses on elites, representation and computational methods. You can find out more about some of my current research projects here.\n Prior to my PhD at the University of Glasgow, I earned a masters in international relations at the University of Edinburgh and a bachelor’s degree in political science at Northeastern Illinois University in my hometown of Chicago.\nA copy of my CV (which probably needs to be updated) can be downloaded here."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Zachary Dickson",
    "section": "",
    "text": "I’m currently a postdoctoral research fellow at the London School of Economics on the COVIDEU project. I’m also an affiliate at the  LSE Data Science Institute. My research focuses on elites, representation and computational methods. You can find out more about some of my current research projects here.\n Prior to my PhD at the University of Glasgow, I earned a masters in international relations at the University of Edinburgh and a bachelor’s degree in political science at Northeastern Illinois University in my hometown of Chicago.\nA copy of my CV (which probably needs to be updated) can be downloaded here."
  }
]